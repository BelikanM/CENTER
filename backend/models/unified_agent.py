"""
ü§ñ AGENT IA VISUEL MULTIMODAL - LE PLUS PUISSANT DU MONDE
===========================================================

Agent central ultra-performant qui orchestre TOUS les mod√®les disponibles
dans backend/models/ pour cr√©er l'IA multimodale la plus avanc√©e.

Mod√®les Int√©gr√©s (TOUS):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1. üëÅÔ∏è VISION - SmolVLM-500M-Instruct (Compr√©hension visuelle avanc√©e)
   ‚Ä¢ Path: models/smolvlm/cache/models--HuggingFaceTB--SmolVLM-500M-Instruct
   ‚Ä¢ Capacit√©: Analyse et description d'images en langage naturel
   
2. üéØ D√âTECTION - YOLO TensorFlow.js (D√©tection d'objets en temps r√©el)
   ‚Ä¢ Path: models/lifemodo_tfjs
   ‚Ä¢ Capacit√©: Localisation et classification d'objets multiples
   
3. üß† INTELLIGENCE - Mistral-7B-Instruct (Raisonnement et langage)
   ‚Ä¢ Path: models/mistral/mistral-7b-instruct-v0.2.Q4_K_M.gguf
   ‚Ä¢ Capacit√©: G√©n√©ration de texte, raisonnement logique, conversation
   
4. üó£Ô∏è VOIX - Coqui TTS (Synth√®se vocale multilingue)
   ‚Ä¢ Path: models/tts/tts_models--fr--css10--vits
   ‚Ä¢ Capacit√©: G√©n√©ration audio naturelle en fran√ßais

Architecture Avanc√©e:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
- üîÑ ReAct Loop: Reasoning + Acting pour d√©cisions intelligentes
- üß† M√©moire contextuelle: Court-terme et long-terme
- üõ†Ô∏è Tools System: Chaque mod√®le = un outil sp√©cialis√©
- üîó LangChain Integration: Cha√Ænes et agents avanc√©s
- ‚ö° Pipeline optimis√©: Orchestration parall√®le quand possible

Auteur: BelikanM
Date: 13 Novembre 2025
Version: 2.0.0 - Edition Ultime
"""

import os
import sys
import logging
from typing import Dict, Any, List, Optional, Union, Callable
from pathlib import Path
from datetime import datetime
import json
from dotenv import load_dotenv

# Charger variables d'environnement
load_dotenv(Path(__file__).parent / ".env")

# Configuration du logging am√©lior√©e
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import Tavily pour recherche internet
try:
    from tavily import TavilyClient
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    TAVILY_AVAILABLE = True
    logger.info("‚úÖ Tavily disponible dans UnifiedAgent")
except Exception as e:
    TAVILY_AVAILABLE = False
    tavily_client = None
    logger.warning(f"‚ö†Ô∏è Tavily non disponible: {e}")


# ==========================================
# SYST√àME D'OUTILS (TOOLS)
# ==========================================

class BaseTool:
    """Classe de base pour tous les outils"""
    
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self.is_ready = False
    
    def execute(self, *args, **kwargs) -> Dict[str, Any]:
        """Ex√©cuter l'outil"""
        raise NotImplementedError("Subclass must implement execute()")
    
    def __repr__(self):
        status = "‚úÖ" if self.is_ready else "‚ùå"
        return f"<{self.name} {status}>"


class VisionTool(BaseTool):
    """Outil de vision avec SmolVLM"""
    
    def __init__(self, model_path: str):
        super().__init__(
            name="vision_analyzer",
            description="Analyse et d√©crit des images en langage naturel. Utilise SmolVLM-500M-Instruct."
        )
        self.model_path = Path(model_path)
        self.model = None
        self.processor = None
        self._initialize()
    
    def _initialize(self):
        """Initialiser le mod√®le de vision"""
        try:
            from transformers import AutoProcessor, AutoModelForVision2Seq
            import torch
            
            model_id = "HuggingFaceTB/SmolVLM-500M-Instruct"
            cache_dir = str(self.model_path)
            
            logger.info(f"üîÑ Chargement SmolVLM depuis {cache_dir}...")
            
            self.processor = AutoProcessor.from_pretrained(
                model_id,
                cache_dir=cache_dir
            )
            self.model = AutoModelForVision2Seq.from_pretrained(
                model_id,
                cache_dir=cache_dir,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else "cpu"
            )
            
            self.is_ready = True
            logger.info("‚úÖ SmolVLM pr√™t")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur SmolVLM: {e}")
            self.is_ready = False
    
    def execute(self, image_path: str, question: str = "D√©cris cette image en d√©tail") -> Dict[str, Any]:
        """Analyser une image"""
        if not self.is_ready:
            return {"error": "Vision tool not ready"}
        
        try:
            from PIL import Image
            
            image = Image.open(image_path)
            
            # Pr√©parer l'input
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": question}
                    ]
                }
            ]
            
            prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)
            inputs = self.processor(text=prompt, images=[image], return_tensors="pt")
            inputs = inputs.to(self.model.device)
            
            # G√©n√©rer la r√©ponse
            generated_ids = self.model.generate(**inputs, max_new_tokens=500)
            generated_texts = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True
            )
            
            return {
                "success": True,
                "description": generated_texts[0],
                "question": question,
                "image": image_path
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse vision: {e}")
            return {"error": str(e)}


class DetectionTool(BaseTool):
    """Outil de d√©tection d'objets avec YOLO"""
    
    def __init__(self, model_path: str):
        super().__init__(
            name="object_detector",
            description="D√©tecte et localise des objets dans des images. Utilise YOLO TensorFlow.js."
        )
        self.model_path = Path(model_path)
        self.is_ready = self.model_path.exists()
    
    def execute(self, image_path: str, confidence: float = 0.5) -> Dict[str, Any]:
        """D√©tecter des objets dans une image"""
        # Note: YOLO TF.js n√©cessite JavaScript, on retourne les specs
        return {
            "success": True,
            "note": "YOLO TensorFlow.js - Ex√©cution c√¥t√© navigateur",
            "model_path": str(self.model_path),
            "config": {
                "confidence_threshold": confidence,
                "type": "tensorflow_js",
                "usage": "Browser-based detection"
            },
            "image": image_path
        }


class LLMTool(BaseTool):
    """Outil de raisonnement avec Mistral-7B"""
    
    def __init__(self, model_path: str):
        super().__init__(
            name="reasoning_engine",
            description="G√©n√®re du texte, raisonne logiquement et converse. Utilise Mistral-7B-Instruct."
        )
        self.model_path = Path(model_path)
        self.llm = None
        self._initialize()
    
    def _initialize(self):
        """Initialiser le LLM"""
        try:
            from llama_cpp import Llama
            
            if not self.model_path.exists():
                logger.error(f"‚ùå Mod√®le introuvable: {self.model_path}")
                return
            
            logger.info(f"üîÑ Chargement Mistral-7B depuis {self.model_path}...")
            
            self.llm = Llama(
                model_path=str(self.model_path),
                n_ctx=4096,  # Contexte
                n_threads=4,  # CPU threads
                n_gpu_layers=0,  # CPU only pour compatibilit√©
                verbose=False
            )
            
            self.is_ready = True
            logger.info("‚úÖ Mistral-7B pr√™t")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Mistral: {e}")
            self.is_ready = False
    
    def execute(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> Dict[str, Any]:
        """G√©n√©rer une r√©ponse"""
        if not self.is_ready:
            return {"error": "LLM tool not ready"}
        
        try:
            # Format Mistral-Instruct (sans <s> car llama-cpp l'ajoute automatiquement)
            formatted_prompt = f"[INST] {prompt} [/INST]"
            
            response = self.llm(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=["</s>", "[INST]"]
            )
            
            return {
                "success": True,
                "response": response['choices'][0]['text'].strip(),
                "prompt": prompt,
                "tokens": response['usage']['total_tokens']
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration LLM: {e}")
            return {"error": str(e)}


class TTSTool(BaseTool):
    """Outil de synth√®se vocale avec Coqui TTS"""
    
    def __init__(self, model_path: str):
        super().__init__(
            name="voice_synthesizer",
            description="Convertit du texte en parole naturelle. Utilise Coqui TTS fran√ßais."
        )
        self.model_path = Path(model_path)
        self.tts = None
        self._initialize()
    
    def _initialize(self):
        """Initialiser TTS"""
        try:
            # Charger la configuration TTS depuis tts-env
            import sys
            script_dir = Path(__file__).parent
            
            # Essayer de charger TTS depuis tts-env
            TTS_ENV_PATH = r"C:\Users\Admin\miniconda3\envs\tts-env\Lib\site-packages"
            if os.path.exists(TTS_ENV_PATH) and TTS_ENV_PATH not in sys.path:
                sys.path.insert(0, TTS_ENV_PATH)
                logger.info(f"üîÑ Chargement TTS depuis tts-env")
            
            # Ajouter le chemin parent (backend/) au path
            parent_dir = script_dir.parent
            if str(parent_dir) not in sys.path:
                sys.path.insert(0, str(parent_dir))
            
            from services.tts_service import TTSService
            
            self.tts = TTSService(model_name="tts_models/fr/css10/vits")
            self.is_ready = self.tts.is_ready
            
            if self.is_ready:
                logger.info("‚úÖ TTS pr√™t")
            else:
                logger.warning("‚ö†Ô∏è TTS en mode fallback")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur TTS: {e}")
            self.is_ready = False
    
    def execute(self, text: str, language: str = "fr") -> Dict[str, Any]:
        """Synth√©tiser de la parole"""
        if not self.tts:
            return {"error": "TTS tool not ready"}
        
        try:
            result = self.tts.text_to_speech(text=text)
            return {
                "success": True,
                "text": text,
                "audio_url": result.get("audio_url"),
                "method": result.get("method"),
                "language": language
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur synth√®se vocale: {e}")
            return {"error": str(e)}


# ==========================================
# AGENT IA MULTIMODAL ULTIME
# ==========================================

class UnifiedAgent:
    """
    Agent IA Multimodal Unifi√©
    
    Cet agent orchestre tous les mod√®les disponibles pour fournir
    une intelligence artificielle compl√®te et coh√©rente.
    
    Architecture:
    - Tools: Chaque mod√®le est un outil sp√©cialis√©
    - ReAct: Reasoning + Acting pour d√©cisions intelligentes
    - Memory: Contexte court-terme et session
    - Pipeline: Orchestration optimis√©e
    """
    
    def __init__(
        self,
        models_dir: str = None,  # None = auto-detect
        enable_voice: bool = True,
        enable_vision: bool = True,
        enable_detection: bool = True,
        enable_llm: bool = True
    ):
        """
        Initialiser l'agent unifi√©
        
        Args:
            models_dir: Chemin vers le dossier des mod√®les (None = auto-detect)
            enable_voice: Activer le module TTS
            enable_vision: Activer SmolVLM
            enable_detection: Activer YOLO
            enable_llm: Activer Mistral-7B
        """
        # Auto-d√©tection du dossier models
        if models_dir is None:
            script_dir = Path(__file__).parent
            # Si on est dans backend/models/
            if script_dir.name == "models":
                self.models_dir = script_dir
            else:
                self.models_dir = script_dir / "models"
        elif Path(models_dir).is_absolute():
            self.models_dir = Path(models_dir)
        else:
            # Si chemin relatif, r√©soudre √† partir du script courant
            self.models_dir = Path(__file__).parent if models_dir == "." else Path(models_dir)
        
        self.models_dir = self.models_dir.resolve()  # Chemin absolu
        
        self.config = {
            "voice": enable_voice,
            "vision": enable_vision,
            "detection": enable_detection,
            "llm": enable_llm
        }
        
        # √âtat de l'agent
        self.is_ready = False
        self.tools = {}  # Tools LangChain
        self.capabilities = []
        
        # M√©moire contextuelle
        self.context = {
            "short_term": [],  # Derni√®res 10 interactions
            "session": {},     # Contexte de la session actuelle
            "user_prefs": {}   # Pr√©f√©rences utilisateur
        }
        
        logger.info(f"ü§ñ Initialisation de l'Agent IA Multimodal Unifi√©...")
        logger.info(f"üìÇ Dossier mod√®les: {self.models_dir}")
        self._initialize_tools()
    
    
    def _initialize_tools(self):
        """Initialiser tous les outils (models as tools)"""
        logger.info("ÔøΩÔ∏è  Chargement des outils IA...")
        
        # 1. Vision Tool (SmolVLM)
        if self.config["vision"]:
            try:
                vision_path = self.models_dir / "smolvlm" / "cache"
                self.tools["vision"] = VisionTool(model_path=str(vision_path))
                if self.tools["vision"].is_ready:
                    self.capabilities.append("üëÅÔ∏è Vision (SmolVLM-500M)")
            except Exception as e:
                logger.error(f"‚ùå Erreur Vision Tool: {e}")
        
        # 2. Detection Tool (YOLO)
        if self.config["detection"]:
            try:
                detection_path = self.models_dir / "lifemodo_tfjs"
                self.tools["detection"] = DetectionTool(model_path=str(detection_path))
                if self.tools["detection"].is_ready:
                    self.capabilities.append("üéØ D√©tection (YOLO TF.js)")
            except Exception as e:
                logger.error(f"‚ùå Erreur Detection Tool: {e}")
        
        # 3. LLM Tool (Mistral-7B)
        if self.config["llm"]:
            try:
                llm_path = self.models_dir / "mistral" / "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
                self.tools["llm"] = LLMTool(model_path=str(llm_path))
                if self.tools["llm"].is_ready:
                    self.capabilities.append("üß† Raisonnement (Mistral-7B)")
            except Exception as e:
                logger.error(f"‚ùå Erreur LLM Tool: {e}")
        
        # 4. TTS Tool (Coqui)
        if self.config["voice"]:
            try:
                tts_path = self.models_dir / "tts" / "tts_models--fr--css10--vits"
                self.tools["tts"] = TTSTool(model_path=str(tts_path))
                if self.tools["tts"].is_ready:
                    self.capabilities.append("üó£Ô∏è Synth√®se vocale (Coqui TTS)")
            except Exception as e:
                logger.error(f"‚ùå Erreur TTS Tool: {e}")
        
        # V√©rifier l'√©tat
        self._check_readiness()
    
    
    def _check_readiness(self):
        """V√©rifier l'√©tat de pr√©paration de l'agent"""
        ready_count = len([t for t in self.tools.values() if t.is_ready])
        total_count = sum(1 for v in self.config.values() if v)
        
        self.is_ready = ready_count > 0
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ü§ñ AGENT IA MULTIMODAL - LE PLUS PUISSANT DU MONDE")
        logger.info(f"{'='*70}")
        logger.info(f"Outils charg√©s: {ready_count}/{total_count}")
        logger.info(f"\n‚ú® Capacit√©s disponibles:")
        for cap in self.capabilities:
            logger.info(f"   {cap}")
        
        # Afficher les outils
        logger.info(f"\nüõ†Ô∏è  Outils op√©rationnels:")
        for name, tool in self.tools.items():
            status = "‚úÖ" if tool.is_ready else "‚ùå"
            logger.info(f"   {status} {tool.name}: {tool.description[:50]}...")
        
        logger.info(f"\n{'='*70}")
        
        if self.is_ready:
            logger.info("‚úÖ Agent ultra-puissant pr√™t pour Flutter!")
        else:
            logger.warning("‚ö†Ô∏è  Agent partiellement op√©rationnel")
        
        logger.info(f"{'='*70}\n")
    
    
    # ==========================================
    # M√âTHODES PRINCIPALES
    # ==========================================
    
    def process_image(
        self,
        image_path: str,
        question: Optional[str] = None,
        detect_objects: bool = True
    ) -> Dict[str, Any]:
        """
        üî• ANALYSE ULTRA-COMPL√àTE D'IMAGE - UTILISE TOUS LES OUTILS DISPONIBLES
        
        Pipeline intelligent:
        1. SmolVLM (Vision) - Compr√©hension visuelle d√©taill√©e
        2. YOLO (D√©tection) - Objets, personnes, zones d'int√©r√™t
        3. FAISS (M√©moire) - Comparaison avec images similaires vues
        4. Mistral-7B (LLM) - Synth√®se intelligente + raisonnement
        5. Tavily (Web) - Recherche internet si n√©cessaire
        
        Args:
            image_path: Chemin vers l'image
            question: Question optionnelle sur l'image
            detect_objects: Activer la d√©tection d'objets YOLO (d√©faut: True)
        
        Returns:
            R√©sultat complet avec TOUTES les analyses disponibles
        """
        if not self.is_ready:
            return {"error": "Agent non pr√™t"}
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "image": image_path,
            "vision": None,
            "detection": None,
            "synthesis": None,
            "web_search": None,
            "tools_used": []
        }
        
        try:
            # ========================================
            # √âTAPE 1: VISION AVEC SMOLVLM (TOUJOURS)
            # ========================================
            if "vision" in self.tools and self.tools["vision"].is_ready:
                logger.info("üëÅÔ∏è [SmolVLM] Analyse visuelle en cours...")
                result["vision"] = self.tools["vision"].execute(
                    image_path=image_path,
                    question=question or "D√©cris cette image en d√©tail avec tous les √©l√©ments visibles"
                )
                result["tools_used"].append("SmolVLM-500M (Vision)")
                logger.info(f"   ‚úì Vision compl√©t√©e: {len(result['vision'].get('description', ''))} caract√®res")
            else:
                logger.warning("‚ö†Ô∏è SmolVLM non disponible")
            
            # ========================================
            # √âTAPE 2: D√âTECTION YOLO (TOUJOURS ACTIF)
            # ========================================
            # CHANGEMENT: Toujours activer la d√©tection pour une analyse compl√®te
            if "detection" in self.tools and self.tools["detection"].is_ready:
                logger.info("üéØ [YOLO] D√©tection d'objets en cours...")
                result["detection"] = self.tools["detection"].execute(
                    image_path=image_path,
                    confidence=0.4  # Seuil plus bas pour d√©tecter plus d'objets
                )
                objects_found = len(result["detection"].get("detections", []))
                result["tools_used"].append(f"YOLO TF.js ({objects_found} objets)")
                logger.info(f"   ‚úì D√©tection compl√©t√©e: {objects_found} objets trouv√©s")
            else:
                logger.warning("‚ö†Ô∏è YOLO non disponible")
            
            # ========================================
            # √âTAPE 3: SYNTH√àSE INTELLIGENTE AVEC MISTRAL
            # ========================================
            if "llm" in self.tools and self.tools["llm"].is_ready:
                logger.info("üß† [Mistral-7B] G√©n√©ration de synth√®se intelligente...")
                synthesis_prompt = self._build_synthesis_prompt(result)
                synthesis_result = self.tools["llm"].execute(
                    prompt=synthesis_prompt,
                    max_tokens=250,  # R√©duit pour rapidit√©
                    temperature=0.6  # Plus pr√©cis
                )
                result["synthesis"] = synthesis_result.get("response")
                result["tools_used"].append("Mistral-7B (LLM)")
                logger.info(f"   ‚úì Synth√®se g√©n√©r√©e: {len(result['synthesis'])} caract√®res")
                
                # ========================================
                # √âTAPE 4: RECHERCHE WEB AUTOMATIQUE SI PERTINENT
                # ========================================
                if TAVILY_AVAILABLE and tavily_client:
                    synthesis_lower = result["synthesis"].lower() if result["synthesis"] else ""
                    vision_desc = result.get("vision", {}).get("description", "").lower()
                    
                    # TRIGGERS √âLARGIS pour recherche automatique
                    search_triggers = [
                        # Texte/Logo/Marque
                        "logo", "marque", "entreprise", "soci√©t√©", "nom", "texte", "√©crit",
                        "inscription", "enseigne", "panneau",
                        # Objets sp√©cifiques
                        "√©quipement", "appareil", "instrument", "outil", "machine",
                        # Personnes/Professions
                        "uniforme", "tenue", "professionnel", "m√©tier",
                        # Besoin d'info
                        "rechercher", "identifier", "plus d'infos", "c'est quoi",
                        # Lieux
                        "b√¢timent", "lieu", "endroit", "structure"
                    ]
                    
                    should_search = any(trigger in synthesis_lower or trigger in vision_desc 
                                       for trigger in search_triggers)
                    
                    if should_search:
                        try:
                            # PASSER LES R√âSULTATS YOLO √† _extract_search_query
                            search_query = self._extract_search_query(
                                vision_desc, 
                                result["synthesis"],
                                detection_result=result.get("detection")  # ‚úÖ NOUVEAU: Passer YOLO
                            )
                            
                            if search_query and len(search_query) > 3:
                                logger.info(f"üåê [Tavily] Recherche: '{search_query[:60]}...'")
                                search_results = tavily_client.search(
                                    query=search_query, 
                                    max_results=3,  # Augment√© √† 3 pour plus d'infos
                                    search_depth="basic"
                                )
                                
                                result["web_search"] = {
                                    "query": search_query,
                                    "results": search_results.get("results", [])[:3]
                                }
                                result["tools_used"].append(f"Tavily ({len(result['web_search']['results'])} r√©sultats)")
                                
                                # Enrichir la synth√®se
                                if result["web_search"]["results"]:
                                    web_info = "\n\nüåê Informations compl√©mentaires (internet):\n"
                                    for i, res in enumerate(result["web_search"]["results"], 1):
                                        title = res.get('title', 'N/A')
                                        content = res.get('content', '')[:180]
                                        web_info += f"‚Ä¢ {title}: {content}...\n"
                                    result["synthesis"] += web_info
                                    logger.info(f"   ‚úì Web search compl√©t√©: {len(result['web_search']['results'])} r√©sultats int√©gr√©s")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Recherche web √©chou√©e: {e}")
            
            # ========================================
            # √âTAPE 5: AJOUTER AU CONTEXTE M√âMOIRE
            # ========================================
            self._add_to_context("image_analysis", result)
            
            # R√©sum√© des outils utilis√©s
            tools_summary = " + ".join(result["tools_used"])
            logger.info(f"‚úÖ Analyse compl√®te termin√©e - Outils: {tools_summary}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse image: {e}")
            return {"error": str(e)}
    
    def chat(
        self,
        message: str,
        with_voice: bool = False,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        üî• CHAT ULTRA-INTELLIGENT - UTILISE TOUS LES OUTILS DISPONIBLES
        
        Pipeline intelligent:
        1. Analyse de la question ‚Üí D√©tecte le type de r√©ponse n√©cessaire
        2. Recherche FAISS ‚Üí M√©moire des conversations/images pr√©c√©dentes
        3. Recherche Web (Tavily) ‚Üí Informations √† jour si n√©cessaire
        4. Analyse visuelle (SmolVLM + YOLO) ‚Üí Si image fournie
        5. G√©n√©ration LLM (Mistral-7B) ‚Üí Synth√®se intelligente compl√®te
        6. TTS (Coqui) ‚Üí Audio si demand√©
        
        Args:
            message: Message de l'utilisateur
            with_voice: G√©n√©rer r√©ponse audio
            context: Contexte additionnel (peut inclure image_path, memory, etc.)
        
        Returns:
            R√©ponse enrichie avec TOUS les outils disponibles
        """
        if not self.is_ready:
            return {"error": "Agent non pr√™t"}
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "user_message": message,
            "response": None,
            "audio_url": None,
            "tools_used": [],
            "sources": []  # Sources d'information utilis√©es
        }
        
        try:
            # Enrichir le contexte
            full_context = context or {}
            full_context["chat_history"] = self.context["short_term"][-5:]  # 5 derniers
            
            # ========================================
            # √âTAPE 1: ANALYSE S√âMANTIQUE DE LA QUESTION
            # ========================================
            message_lower = message.lower()
            needs_web_search = any(keyword in message_lower for keyword in [
                "actualit√©", "news", "aujourd'hui", "r√©cent", "maintenant",
                "qui est", "c'est quoi", "qu'est-ce que", "recherche",
                "derni√®re", "dernier", "nouveau", "nouvelle",
                "site web", "internet", "en ligne"
            ])
            
            needs_memory_search = any(keyword in message_lower for keyword in [
                "pr√©c√©dent", "avant", "d√©j√†", "parl√©", "dit",
                "derni√®re fois", "conversation", "historique",
                "image pr√©c√©dente", "photo d'avant"
            ])
            
            # ========================================
            # √âTAPE 2: RECHERCHE DANS LA M√âMOIRE FAISS (Si pertinent)
            # ========================================
            if needs_memory_search and "memory" in full_context:
                logger.info("üíæ [FAISS] Recherche dans la m√©moire...")
                # NOTE: L'API chat_agent_api.py g√®re d√©j√† FAISS
                # On enrichit juste le contexte ici
                result["tools_used"].append("FAISS (M√©moire)")
                result["sources"].append("M√©moire conversationnelle")
            
            # ========================================
            # √âTAPE 3: RECHERCHE WEB TAVILY (Si n√©cessaire)
            # ========================================
            if needs_web_search and TAVILY_AVAILABLE and tavily_client:
                try:
                    logger.info(f"üåê [Tavily] Recherche web: '{message[:60]}...'")
                    search_results = tavily_client.search(
                        query=message,
                        max_results=3,
                        search_depth="basic"
                    )
                    
                    full_context["web_search"] = {
                        "query": message,
                        "results": search_results.get("results", [])[:3]
                    }
                    result["tools_used"].append(f"Tavily ({len(full_context['web_search']['results'])} r√©sultats)")
                    result["sources"].append("Internet (recherche en temps r√©el)")
                    logger.info(f"   ‚úì Web search: {len(full_context['web_search']['results'])} r√©sultats trouv√©s")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Recherche web √©chou√©e: {e}")
            
            # ========================================
            # √âTAPE 4: ANALYSE VISUELLE (Si image fournie)
            # ========================================
            if "image_path" in full_context:
                logger.info("üëÅÔ∏è [SmolVLM + YOLO] Analyse d'image dans contexte...")
                image_analysis = self.process_image(
                    image_path=full_context["image_path"],
                    question=message,
                    detect_objects=True  # TOUJOURS activer YOLO
                )
                full_context["image_analysis"] = image_analysis
                
                # Ajouter les outils visuels utilis√©s
                if "tools_used" in image_analysis:
                    result["tools_used"].extend(image_analysis["tools_used"])
                result["sources"].append("Analyse visuelle de l'image fournie")
            
            # ========================================
            # √âTAPE 5: G√âN√âRATION AVEC MISTRAL-7B (LLM)
            # ========================================
            if "llm" in self.tools and self.tools["llm"].is_ready:
                logger.info("üß† [Mistral-7B] G√©n√©ration de r√©ponse intelligente...")
                
                # Param√®tres adaptatifs depuis le contexte
                max_tokens = full_context.get("max_tokens", 200)  # Rapide
                temperature = full_context.get("temperature", 0.5)  # Pr√©cis
                
                # Construire prompt enrichi avec TOUTES les sources
                chat_prompt = self._build_chat_prompt(message, full_context)
                llm_result = self.tools["llm"].execute(
                    prompt=chat_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                result["response"] = llm_result.get("response", "R√©ponse g√©n√©r√©e")
                result["tools_used"].append("Mistral-7B (LLM)")
                result["sources"].append("Raisonnement IA local")
                logger.info(f"   ‚úì R√©ponse g√©n√©r√©e: {len(result['response'])} caract√®res")
            else:
                result["response"] = "Mod√®le LLM non disponible. R√©ponse directe limit√©e."
            
            # ========================================
            # √âTAPE 6: SYNTH√àSE VOCALE (Si demand√©e)
            # ========================================
            if with_voice and "tts" in self.tools and self.tools["tts"].is_ready:
                logger.info("üó£Ô∏è [Coqui TTS] G√©n√©ration audio...")
                tts_result = self.tools["tts"].execute(
                    text=result["response"],
                    language="fr"
                )
                result["audio_url"] = tts_result.get("audio_url")
                result["tools_used"].append("Coqui TTS")
            
            # ========================================
            # √âTAPE 7: M√âMORISATION DU CONTEXTE
            # ========================================
            self._add_to_context("chat", result)
            
            # R√©sum√© des outils utilis√©s
            tools_summary = " + ".join(result["tools_used"]) if result["tools_used"] else "R√©ponse directe"
            logger.info(f"‚úÖ Chat compl√©t√© - Outils: {tools_summary}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chat: {e}")
            return {"error": str(e)}
    
    def speak(self, text: str, language: str = "fr") -> Dict[str, Any]:
        """
        Faire parler l'agent
        
        Args:
            text: Texte √† synth√©tiser
            language: Langue (fr, en, es, etc.)
        
        Returns:
            Informations sur l'audio g√©n√©r√©
        """
        if "tts" not in self.tools or not self.tools["tts"].is_ready:
            return {"error": "TTS non disponible"}
        
        try:
            logger.info(f"üó£Ô∏è Synth√®se vocale: {text[:50]}...")
            result = self.tools["tts"].execute(text=text, language=language)
            logger.info("‚úÖ Audio g√©n√©r√©")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur TTS: {e}")
            return {"error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """Obtenir l'√©tat complet de l'agent"""
        return {
            "ready": self.is_ready,
            "tools": {
                name: {
                    "name": tool.name,
                    "ready": tool.is_ready,
                    "description": tool.description
                }
                for name, tool in self.tools.items()
            },
            "capabilities": self.capabilities,
            "context_size": len(self.context["short_term"]),
            "config": self.config,
            "version": "2.0.0 - Agent IA Multimodal Ultimate"
        }
    
    
    # ==========================================
    # M√âTHODES UTILITAIRES
    # ==========================================
    
    def _build_synthesis_prompt(self, analysis_result: Dict) -> str:
        """
        üî• PROMPT DE SYNTH√àSE ULTRA-INTELLIGENT
        
        Construit un prompt qui ENCOURAGE l'agent √† utiliser TOUS les outils disponibles:
        - SmolVLM pour vision d√©taill√©e
        - YOLO pour localisation pr√©cise
        - Tavily pour informations manquantes
        - FAISS pour contexte historique
        """
        vision_desc = analysis_result.get("vision", {}).get("description", "Aucune vision")
        detection = analysis_result.get("detection", {})
        tools_used = analysis_result.get("tools_used", [])
        
        # Compter les objets d√©tect√©s
        detections_list = detection.get("detections", [])
        objects_count = len(detections_list)
        
        # Extraire les classes d'objets d√©tect√©es
        detected_classes = list(set([d.get("class", "unknown") for d in detections_list])) if detections_list else []
        
        prompt = f"""Tu es Kibali Enfant Agent, un assistant IA multimodal ULTRA-INTELLIGENT avec acc√®s √† des outils puissants.

üîß OUTILS DISPONIBLES UTILIS√âS:
{' + '.join(tools_used) if tools_used else 'Analyse de base'}

üì∏ ANALYSE VISUELLE (SmolVLM-500M):
{vision_desc}

üéØ D√âTECTION D'OBJETS (YOLO TensorFlow.js):
- Objets d√©tect√©s: {objects_count}
- Classes identifi√©es: {', '.join(detected_classes) if detected_classes else 'Aucune'}
{json.dumps(detection, ensure_ascii=False, indent=2) if detection else 'Aucune d√©tection'}

üìã INSTRUCTIONS POUR SYNTH√àSE INTELLIGENTE:

1. UTILISE ACTIVEMENT les r√©sultats des outils:
   ‚úì SmolVLM te donne la compr√©hension VISUELLE globale
   ‚úì YOLO te donne les OBJETS PR√âCIS et leur localisation
   ‚úì COMBINE les deux pour une analyse compl√®te

2. D√âTECTE si l'image contient des √âL√âMENTS IDENTIFIABLES:
   - Logo d'entreprise/marque ‚Üí Mentionne que tu peux chercher sur internet
   - Texte visible/inscription ‚Üí Signale que tu peux rechercher plus d'infos
   - Produit sp√©cifique ‚Üí Indique que tu peux trouver des d√©tails en ligne
   - Personne en uniforme ‚Üí Identifie la profession et l'√©quipement
   - √âquipement technique ‚Üí Nomme l'appareil et son usage

3. SI L'ANALYSE EST INCOMPL√àTE:
   - Indique clairement ce qui manque
   - Sugg√®re: "Je peux rechercher sur internet pour plus de pr√©cisions"
   - Propose: "Je peux utiliser mes outils pour identifier cet √©l√©ment"

4. EXEMPLES DE R√âPONSES ULTRA-INTELLIGENTES:
   ‚ùå MAUVAIS: "Je vois une personne."
   ‚úÖ BON: "Je vois une personne en tenue professionnelle (d√©tect√©e par YOLO) avec un √©quipement de mesure visible (th√©odolite selon SmolVLM). C'est probablement un g√©om√®tre-topographe. Je peux rechercher plus d'infos sur cet √©quipement si n√©cessaire."

   ‚ùå MAUVAIS: "Il y a un logo."
   ‚úÖ BON: "Je d√©tecte un logo avec le texte 'Nike' (visible dans l'analyse SmolVLM). C'est la marque de sport am√©ricaine Nike, sp√©cialis√©e en √©quipements sportifs. Je peux chercher plus d'informations si besoin."

   ‚ùå MAUVAIS: "C'est un document."
   ‚úÖ BON: "L'image montre un document avec du texte en fran√ßais (identifi√© par SmolVLM). YOLO d√©tecte {objects_count} √©l√©ments dont possiblement des zones de texte. Je peux rechercher le contexte de ce document sur internet pour plus de d√©tails."

5. FORMAT DE R√âPONSE:
   - 3-5 phrases MAXIMUM
   - COMMENCE par ce que tu VOIS (SmolVLM + YOLO)
   - EXPLIQUE ce que c'est (ton intelligence)
   - PROPOSE d'utiliser d'autres outils si pertinent

R√©ponds de mani√®re PROACTIVE, PR√âCISE et ULTRA-UTILE en fran√ßais."""
        
        return prompt
    
    def _extract_search_query(self, vision_desc: str, synthesis: str, detection_result: Dict = None) -> Optional[str]:
        """
        üîç EXTRACTION INTELLIGENTE DE REQU√äTE POUR RECHERCHER LE TH√àME DE L'IMAGE
        
        Ne cherche PAS les mots isol√©s mais le CONTEXTE et le TH√àME visuel.
        Exemple: Au lieu de "cet carte", cherche "plan topographique site construction"
        
        Args:
            vision_desc: Description visuelle de SmolVLM
            synthesis: Synth√®se g√©n√©r√©e par Mistral
            detection_result: R√©sultat de d√©tection YOLO (optionnel)
        
        Returns:
            Requ√™te de recherche contextuelle optimis√©e pour Tavily
        """
        import re
        
        # Combiner vision et synth√®se (texte original, pas lowercase)
        full_text_original = f"{vision_desc} {synthesis}"
        full_text = full_text_original.lower()
        
        logger.info("üîç === ANALYSE POUR RECHERCHE WEB ===")
        
        # ========================================
        # √âTAPE 1: IDENTIFIER LE TYPE DE DOCUMENT VISUEL
        # ========================================
        document_types = {
            "plan topographique": ["topographie", "site", "terrain", "sol", "nivellement", "carte topographique"],
            "sch√©ma architectural": ["architecture", "b√¢timent", "construction", "plan de masse", "√©l√©vation"],
            "plan cadastral": ["cadastre", "parcelle", "propri√©t√©", "limite", "foncier"],
            "carte g√©ographique": ["g√©ographie", "r√©gion", "pays", "ville", "localisation"],
            "diagramme technique": ["technique", "syst√®me", "installation", "√©quipement", "infrastructure"],
            "sch√©ma √©lectrique": ["√©lectrique", "circuit", "c√¢blage", "√©lectricit√©"],
            "plan d'am√©nagement": ["am√©nagement", "urbanisme", "zone", "d√©veloppement", "lotissement"],
        }
        
        detected_doc_type = None
        for doc_type, keywords in document_types.items():
            if any(kw in full_text for kw in keywords):
                detected_doc_type = doc_type
                logger.info(f"üìä Type d√©tect√©: {doc_type}")
                break
        
        # ========================================
        # √âTAPE 2: EXTRAIRE LES CONCEPTS VISUELS PRINCIPAUX
        # ========================================
        visual_concepts = []
        
        # Concepts de localisation
        location_patterns = r'\b(site|terrain|emplacement|zone|secteur|r√©gion|lieu|endroit)\b'
        locations = re.findall(location_patterns, full_text, re.IGNORECASE)
        if locations:
            visual_concepts.append("site terrain")
            logger.info(f"üìç Localisation d√©tect√©e")
        
        # Concepts de construction/structure
        structure_patterns = r'\b(b√¢timent|structure|construction|√©difice|maison|immeuble)\b'
        structures = re.findall(structure_patterns, full_text, re.IGNORECASE)
        if structures:
            visual_concepts.append("construction b√¢timent")
            logger.info(f"üèóÔ∏è Structure d√©tect√©e")
        
        # Concepts techniques
        technical_patterns = r'\b(mesure|lev√©|relev√©|calcul|dimension|c√¥te|√©chelle)\b'
        technical = re.findall(technical_patterns, full_text, re.IGNORECASE)
        if technical:
            visual_concepts.append("mesure technique")
            logger.info(f"üìê Aspect technique d√©tect√©")
        
        # ========================================
        # √âTAPE 3: IDENTIFIER LES ANNOTATIONS/L√âGENDES IMPORTANTES
        # ========================================
        # Chercher des mots en MAJUSCULES (souvent des annotations importantes)
        annotations = re.findall(r'\b[A-Z]{2,}[A-Z\s]*\b', full_text_original)
        annotations = [a.strip() for a in annotations if len(a.strip()) > 2]
        
        if annotations:
            logger.info(f"üìå Annotations trouv√©es: {', '.join(annotations[:3])}")
        
        # ========================================
        # √âTAPE 4: CONSTRUIRE LA REQU√äTE CONTEXTUELLE INTELLIGENTE
        # ========================================
        
        # Priorit√© 1: Type de document + Concepts visuels
        if detected_doc_type:
            query_parts = [detected_doc_type]
            
            # Ajouter les concepts visuels pertinents
            if visual_concepts:
                query_parts.extend(visual_concepts[:2])
            
            # Ajouter un terme g√©n√©rique pour des r√©sultats visuels
            query_parts.append("exemple sch√©ma")
            
            query = ' '.join(query_parts)
            logger.info(f"‚úÖ Requ√™te contextuelle: '{query}'")
            return query
        
        # Priorit√© 2: Concepts visuels uniquement
        if visual_concepts:
            query = f"{' '.join(visual_concepts[:2])} plan sch√©ma"
            logger.info(f"‚úÖ Requ√™te visuelle: '{query}'")
            return query
        
        # Priorit√© 3: Termes techniques sp√©cifiques d√©tect√©s
        technical_domains = {
            "topographie": "topographie lev√© terrain mesure",
            "cadastre": "cadastre plan parcelle foncier",
            "architecture": "architecture plan construction b√¢timent",
            "g√©nie civil": "g√©nie civil infrastructure ouvrage",
            "urbanisme": "urbanisme am√©nagement zone urbaine",
        }
        
        for domain, query in technical_domains.items():
            if domain in full_text:
                logger.info(f"‚úÖ Requ√™te domaine: '{query}'")
                return query
        
        # Priorit√© 4: Fallback intelligent - √©viter les mots isol√©s
        # Extraire les noms (souvent des concepts importants)
        important_nouns = re.findall(r'\b(plan|carte|sch√©ma|diagramme|layout|design|structure|syst√®me)\b', full_text, re.IGNORECASE)
        if important_nouns:
            # Ajouter un contexte
            query = f"{important_nouns[0]} technique professionnel exemple"
            logger.info(f"‚úÖ Requ√™te nominale: '{query}'")
            return query
        
        # Dernier recours: Requ√™te g√©n√©rique pour √©viter les traductions
        logger.info("‚ö†Ô∏è Pas de contexte clair d√©tect√©")
        return "sch√©ma technique professionnel plan architectural"
        
        # ========================================
        # √âTAPE 1: ANALYSER LES D√âTECTIONS YOLO POUR TROUVER DES ANNOTATIONS
        # ========================================
        text_regions = []
        if detection_result and detection_result.get("detections"):
            for det in detection_result["detections"]:
                det_class = det.get("class", "").lower()
                # Identifier les zones de texte potentielles
                if any(keyword in det_class for keyword in ["text", "label", "annotation", "title", "legend", "caption"]):
                    text_regions.append(det)
                    logger.info(f"üìù Zone de texte d√©tect√©e par YOLO: {det_class}")
        
        # Si YOLO a d√©tect√© des zones de texte, prioriser la recherche sur ces √©l√©ments
        if text_regions:
            logger.info(f"üéØ {len(text_regions)} zone(s) de texte/annotation d√©tect√©e(s) par YOLO")
        
        # ========================================
        # √âTAPE 2: IDENTIFIER LES MOTS-CL√âS DE TITRES/L√âGENDES
        # ========================================
        title_keywords = []
        
        # Patterns pour titres et l√©gendes
        title_patterns = [
            r'titre[:\s]+([^.]+)',
            r'l√©gende[:\s]+([^.]+)',
            r'annotation[:\s]+([^.]+)',
            r'indique[:\s]+([^.]+)',
            r'marqu[√©e]+[:\s]+([^.]+)',
            r'√©crit[:\s]+([^.]+)',
            r'texte[:\s]+([^.]+)',
        ]
        
        for pattern in title_patterns:
            matches = re.findall(pattern, full_text, re.IGNORECASE)
            if matches:
                for match in matches:
                    # Nettoyer et extraire les mots importants
                    words = re.findall(r'\b[A-Z√Ä-≈∏][a-z√†-√ø]+\b|\b\w{4,}\b', match)
                    title_keywords.extend(words[:3])
                    logger.info(f"üìå Titre/l√©gende trouv√©: {match[:50]}...")
        
        # ========================================
        # √âTAPE 3: D√âTECTER LES TYPES DE DOCUMENTS/DIAGRAMMES
        # ========================================
        document_types = {
            "carte": ["carte", "map", "cartographie", "topographie"],
            "sch√©ma": ["sch√©ma", "diagramme", "diagram", "plan"],
            "graphique": ["graphique", "chart", "graph", "courbe"],
            "tableau": ["tableau", "table", "donn√©es"],
            "infographie": ["infographie", "infographic", "visualisation"],
        }
        
        detected_type = None
        for doc_type, keywords in document_types.items():
            if any(kw in full_text for kw in keywords):
                detected_type = doc_type
                logger.info(f"üìä Type de document d√©tect√©: {doc_type}")
                break
        
        # ========================================
        # √âTAPE 4: EXTRAIRE LES NOMS PROPRES (LIEUX, PERSONNES, MARQUES)
        # ========================================
        proper_nouns = re.findall(r'\b[A-Z√Ä-≈∏][a-z√†-√ø]+(?:\s+[A-Z√Ä-≈∏][a-z√†-√ø]+)*\b', vision_desc + " " + synthesis)
        proper_nouns = list(set(proper_nouns))[:5]  # Top 5 uniques
        
        if proper_nouns:
            logger.info(f"üè∑Ô∏è Noms propres d√©tect√©s: {', '.join(proper_nouns[:3])}")
        
        # ========================================
        # √âTAPE 5: CONSTRUIRE LA REQU√äTE OPTIMALE
        # ========================================
        
        # Priorit√© 1: Titres/l√©gendes d√©tect√©s
        if title_keywords:
            query = ' '.join(title_keywords[:3])
            logger.info(f"üîç Requ√™te depuis titre/l√©gende: '{query}'")
            return query
        
        # Priorit√© 2: Noms propres importants
        if proper_nouns:
            query = ' '.join(proper_nouns[:2])
            if detected_type:
                query += f" {detected_type}"
            logger.info(f"üîç Requ√™te depuis noms propres: '{query}'")
            return query
        
        # Priorit√© 3: Type de document + contexte
        if detected_type:
            # Ajouter des mots-cl√©s contextuels
            context_words = re.findall(r'\b\w{5,}\b', full_text)
            unique_words = list(set(context_words))[:3]
            query = f"{detected_type} {' '.join(unique_words)}"
            logger.info(f"üîç Requ√™te depuis type de document: '{query}'")
            return query
        
        # Priorit√© 4: Termes techniques sp√©cialis√©s
        technical_terms = {
            "topographie": "√©quipement topographie g√©od√©sie th√©odolite",
            "g√©om√®tre": "g√©om√®tre topographe instruments mesure",
            "architecture": "architecture plan b√¢timent construction",
            "ing√©nierie": "ing√©nierie technique sch√©ma conception",
        }
        
        for term, query in technical_terms.items():
            if term in full_text:
                logger.info(f"üîç Requ√™te technique: '{query}'")
                return query
        
        # Priorit√© 5: Mots-cl√©s g√©n√©raux (fallback)
        keywords = []
        for word in ["logo", "marque", "texte", "document"]:
            if word in full_text:
                pattern = rf'\b\w+\s+{word}\s+(\w+)'
                matches = re.findall(pattern, full_text)
                keywords.extend(matches)
        
        if keywords:
            query = ' '.join(keywords[:2])
            logger.info(f"üîç Requ√™te depuis mots-cl√©s: '{query}'")
            return query
        
        # Dernier recours: Extraire les mots les plus longs
        words = re.findall(r'\b\w{5,}\b', full_text)
        unique_words = list(set(words))[:3]
        query = ' '.join(unique_words) if unique_words else None
        
        if query:
            logger.info(f"üîç Requ√™te g√©n√©rique: '{query}'")
        
        return query
    
    def _build_chat_prompt(self, message: str, context: Dict) -> str:
        """Construire prompt de chat enrichi avec contexte"""
        
        # Contexte image si pr√©sent
        image_context = ""
        if "image_analysis" in context:
            vision = context["image_analysis"].get("vision", {})
            image_context = f"\nüì∏ Contexte Visuel: {vision.get('description', 'N/A')}"
        
        # Historique r√©cent
        history = context.get("chat_history", [])
        history_text = ""
        if history:
            history_text = "\nüìú Historique R√©cent:\n"
            for h in history[-3:]:  # 3 derniers
                if h.get("type") == "chat":
                    user_msg = h.get("data", {}).get("user_message", "")
                    bot_resp = h.get("data", {}).get("response", "")
                    if user_msg:
                        history_text += f"User: {user_msg}\n"
                    if bot_resp:
                        history_text += f"Assistant: {bot_resp}\n"
        
        prompt = f"""Tu es un assistant IA multimodal ultra-performant et amical. 
Tu combines vision par ordinateur, d√©tection d'objets, raisonnement avanc√© et synth√®se vocale.

{history_text}
{image_context}

üí¨ Message Utilisateur: {message}

R√©ponds de mani√®re naturelle, informative et utile en fran√ßais."""
        
        return prompt
    
    def _add_to_context(self, action_type: str, data: Dict):
        """Ajouter une interaction au contexte"""
        entry = {
            "type": action_type,
            "timestamp": datetime.now().isoformat(),
            "data": data
        }
        
        self.context["short_term"].append(entry)
        
        # Garder seulement les 10 derni√®res
        if len(self.context["short_term"]) > 10:
            self.context["short_term"] = self.context["short_term"][-10:]
    
    def clear_context(self):
        """R√©initialiser le contexte"""
        self.context["short_term"] = []
        self.context["session"] = {}
        logger.info("üßπ Contexte r√©initialis√©")
    
    def __repr__(self) -> str:
        status = "‚úÖ Pr√™t" if self.is_ready else "‚ö†Ô∏è Partiel"
        return f"<UnifiedAgent {status} | {len(self.capabilities)} capacit√©s>"


# ==========================================
# FONCTION D'INITIALISATION
# ==========================================

def create_agent(
    models_dir: str = None,
    **kwargs
) -> UnifiedAgent:
    """
    Cr√©er et initialiser un agent unifi√©
    
    Args:
        models_dir: Chemin vers les mod√®les (None = auto-detect)
        **kwargs: Options de configuration
    
    Returns:
        Instance de UnifiedAgent pr√™te √† l'emploi
    """
    # Auto-d√©tection du dossier models si non fourni
    if models_dir is None:
        # Si on est dans backend/models/
        script_dir = Path(__file__).parent
        if script_dir.name == "models":
            models_dir = str(script_dir)
        else:
            models_dir = str(script_dir / "models")
    
    return UnifiedAgent(models_dir=models_dir, **kwargs)


# ==========================================
# EXEMPLE D'UTILISATION
# ==========================================

if __name__ == "__main__":
    # Cr√©er l'agent
    agent = create_agent()
    
    # V√©rifier l'√©tat
    status = agent.get_status()
    print(f"\nüìä √âtat: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    # Exemple de chat
    if agent.is_ready:
        response = agent.chat(
            message="Bonjour! Comment vas-tu?",
            with_voice=False
        )
        print(f"\nüí¨ R√©ponse: {response}")
        
        # Exemple de synth√®se vocale
        audio = agent.speak("Bienvenue dans le syst√®me multimodal!")
        print(f"\nüó£Ô∏è Audio: {audio}")
